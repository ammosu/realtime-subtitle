#!/usr/bin/env python3
"""
Real-time subtitle overlayï¼ˆLinux/Windowsï¼‰ã€‚

Usage:
    python subtitle_client.py --asr-server http://<SERVER_IP>:8000 --openai-api-key sk-...

Requirements:
    pip install sounddevice numpy scipy requests openai
"""
import argparse
import multiprocessing
import os
import queue
import subprocess
import sys
import threading
import time
from abc import ABC, abstractmethod
from typing import Callable

import numpy as np
import requests
import scipy.signal as signal
import tkinter as tk
from openai import OpenAI


# ---------------------------------------------------------------------------
# ASR Client
# ---------------------------------------------------------------------------

class ASRClient:
    """HTTP client for Qwen3-ASR server."""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip("/")

    def transcribe(self, audio_float32: np.ndarray) -> dict:
        """
        One-shot è½‰éŒ„ï¼šé€å‡ºæ•´æ®µ 16kHz float32 éŸ³è¨Šï¼Œå›å‚³ {"language": str, "text": str}ã€‚
        audio_float32: shape (N,), dtype float32
        """
        r = requests.post(
            f"{self.base_url}/api/transcribe",
            data=audio_float32.tobytes(),
            headers={"Content-Type": "application/octet-stream"},
            timeout=20,
        )
        r.raise_for_status()
        return r.json()


# ---------------------------------------------------------------------------
# Translation Debouncer
# ---------------------------------------------------------------------------

class TranslationDebouncer:
    """
    å°‡è‹±æ–‡ ASR æ–‡å­— debounce å¾Œé€ GPT-4o mini ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        def on_translation(zh_text):
            print(zh_text)

        debouncer = TranslationDebouncer(api_key="sk-...", callback=on_translation)
        debouncer.update("Hello world")  # æ¯æ¬¡ ASR æ›´æ–°æ™‚å‘¼å«
        debouncer.shutdown()
    """

    SENTENCE_ENDINGS = {".", "?", "!", "ã€‚", "ï¼Ÿ", "ï¼"}
    DEBOUNCE_SEC = 0.4

    def __init__(self, api_key: str, callback, model: str = "gpt-4o-mini"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.callback = callback
        self.direction: str = "enâ†’zh"   # ç›®å‰ç¿»è­¯æ–¹å‘

        self._last_translated = ""
        self._pending_text = ""
        self._timer: threading.Timer | None = None
        self._lock = threading.Lock()

    def update(self, text: str):
        """æ¯æ¬¡ ASR æ›´æ–°æ™‚å‘¼å«ã€‚text æ˜¯ç›®å‰çš„å®Œæ•´è½‰éŒ„æ–‡å­—ã€‚"""
        translate_now = None
        with self._lock:
            if text == self._pending_text:
                return
            self._pending_text = text

            # å¥å°¾ç«‹å³ç¿»è­¯ï¼ˆæ³¨æ„ï¼š_do_translate å¿…é ˆåœ¨ lock é‡‹æ”¾å¾Œå‘¼å«ï¼‰
            if text and text[-1] in self.SENTENCE_ENDINGS:
                self._cancel_timer()
                translate_now = text
            else:
                # ä¸€èˆ¬ debounce
                self._cancel_timer()
                self._timer = threading.Timer(self.DEBOUNCE_SEC, self._on_timer)
                self._timer.daemon = True
                self._timer.start()

        # lock å·²é‡‹æ”¾ï¼Œæ‰å¯å‘¼å« OpenAIï¼ˆå¦å‰‡ _do_translate å…§çš„ with self._lock æœƒæ­»é–ï¼‰
        if translate_now:
            self._do_translate(translate_now)

    def _cancel_timer(self):
        if self._timer:
            self._timer.cancel()
            self._timer = None

    def _on_timer(self):
        with self._lock:
            text = self._pending_text
        self._do_translate(text)

    def toggle_direction(self) -> str:
        """åˆ‡æ›ç¿»è­¯æ–¹å‘ï¼Œå›å‚³æ–°æ–¹å‘å­—ä¸²ã€‚"""
        with self._lock:
            self.direction = "zhâ†’en" if self.direction == "enâ†’zh" else "enâ†’zh"
            self._last_translated = ""  # æ¸…ç©ºå¿«å–ï¼Œå¼·åˆ¶é‡æ–°ç¿»è­¯
            return self.direction

    def set_direction(self, direction: str) -> None:
        """ç›´æ¥è¨­å®šæ–¹å‘ï¼ˆ'enâ†’zh' æˆ– 'zhâ†’en'ï¼‰ã€‚"""
        with self._lock:
            self.direction = direction
            self._last_translated = ""

    def _do_translate(self, text: str):
        with self._lock:
            if not text or text == self._last_translated:
                return
            self._last_translated = text
            direction = self.direction  # snapshot
        # lock é‡‹æ”¾å¾Œæ‰å‘¼å« OpenAI
        if direction == "enâ†’zh":
            system_msg = (
                "ä½ æ˜¯å³æ™‚å­—å¹•ç¿»è­¯å“¡ã€‚å°‡è‹±æ–‡èªéŸ³è½‰éŒ„ç¿»è­¯æˆè‡ªç„¶æµæš¢çš„ç¹é«”ä¸­æ–‡ï¼ˆå°ç£å£èªç”¨èªï¼‰ã€‚"
                "è¦æ±‚ï¼š\n"
                "1. ä¾ç…§ä¸­æ–‡èªæ³•é‡æ–°çµ„å¥ï¼Œä¸è¦é€å­—ç¿»è­¯æˆ–ç…§æ¬è‹±æ–‡èªåº\n"
                "2. ä½¿ç”¨å°ç£äººæ—¥å¸¸èªªè©±çš„æ–¹å¼ï¼Œå£èªè‡ªç„¶\n"
                "3. å°ˆæœ‰åè©ã€äººåã€å“ç‰Œå¯ä¿ç•™è‹±æ–‡åŸæ–‡\n"
                "4. åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸åŠ ä»»ä½•è§£é‡‹æˆ–æ¨™æ³¨"
            )
        else:  # zhâ†’en
            system_msg = (
                "You are a real-time subtitle translator. "
                "Translate the Chinese speech transcript to natural, colloquial English. "
                "Output ONLY the translation, no explanations."
            )
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": text},
                ],
                max_tokens=200,
                temperature=0.1,
            )
            translated = response.choices[0].message.content.strip()
            print(f"[Translation] {translated}", flush=True)
            self.callback(translated)
        except Exception as e:
            print(f"[Translation error] {e}", flush=True)

    def shutdown(self):
        with self._lock:
            self._cancel_timer()


# ---------------------------------------------------------------------------
# Subtitle Overlay Window
# ---------------------------------------------------------------------------

class SubtitleOverlay:
    """
    Always-on-top åŠé€æ˜å­—å¹•è¦–çª—ï¼Œå›ºå®šåœ¨æŒ‡å®šè¢å¹•åº•éƒ¨ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        overlay = SubtitleOverlay(screen_index=0)
        overlay.set_text(original="Hello world", translated="ä½ å¥½ä¸–ç•Œ")
        overlay.run()  # é˜»å¡ï¼Œåœ¨ä¸»åŸ·è¡Œç·’å‘¼å«
    """

    TOOLBAR_HEIGHT = 28
    DRAG_BAR_HEIGHT = 14
    WINDOW_HEIGHT = 160          # DRAG_BAR_HEIGHT + 146 (å­—å¹•å€)
    WINDOW_WIDTH = 900           # é è¨­å€¼ï¼Œ__init__ æœƒä¾è¢å¹•å‹•æ…‹è¦†è“‹
    RESIZE_SIZE = 28
    TOOLBAR_BG = "#222222"
    DRAG_BAR_COLOR = "#2a2a2a"   # æ·±ç°ï¼Œéç´”é»‘ï¼ˆä¸æœƒè¢« transparentcolor ç©¿é€ï¼‰
    BTN_COLOR = "#ffffff"
    BTN_BG = "#333333"
    BG_COLOR = "#000000"
    EN_COLOR = "#dddddd"
    ZH_COLOR = "#ffffff"
    SHADOW_COLOR = "#111111"
    EN_FONT = ("Arial", 15)
    ZH_FONT = ("Microsoft JhengHei", 22, "bold")  # Windows ç¹ä¸­å­—é«”

    def __init__(self, screen_index: int = 0, on_toggle_direction=None, on_switch_source=None):
        self._on_toggle_direction = on_toggle_direction
        self._on_switch_source = on_switch_source

        self._root = tk.Tk()

        # ç”¨ tkinter å–è¢å¹•å°ºå¯¸ï¼ˆä¸ä¾è³´ screeninfoï¼‰
        screen_w = self._root.winfo_screenwidth()
        screen_h = self._root.winfo_screenheight()
        # è¦–çª—å¯¬åº¦ç‚ºè¢å¹•çš„ 80%ï¼ˆæœ€å° 900ï¼‰ï¼Œé«˜åº¦å›ºå®š
        self._win_w = max(900, int(screen_w * 0.80))
        self._win_h = self.WINDOW_HEIGHT
        self._x = (screen_w - self._win_w) // 2
        self._y = screen_h - self._win_h - 40

        self._root.overrideredirect(True)
        self._root.wm_attributes("-topmost", True)
        if sys.platform == "win32":
            self._root.wm_attributes("-transparentcolor", self.BG_COLOR)
        else:
            self._root.wm_attributes("-alpha", 0.85)
        self._root.configure(bg=self.BG_COLOR)
        self._root.geometry(
            f"{self._win_w}x{self._win_h}+{self._x}+{self._y}"
        )

        # â”€â”€ æ‹–æ‹‰æ¢ï¼ˆå¸¸é§é ‚éƒ¨ï¼Œæä¾›æ‹–æ‹‰æ§é»ï¼‰ â”€â”€
        drag_bar = tk.Frame(
            self._root,
            bg=self.DRAG_BAR_COLOR,
            height=self.DRAG_BAR_HEIGHT,
            cursor="fleur",          # åå­—ç®­é ­æ¸¸æ¨™ï¼Œæç¤ºå¯æ‹–æ‹‰
        )
        drag_bar.pack(fill="x", side="top")
        drag_bar.pack_propagate(False)
        # æ‹–æ‹‰ç¶å®šåœ¨æ‹–æ‹‰æ¢ä¸Šï¼Œä¸å½±éŸ¿å­—å¹•å€
        drag_bar.bind("<ButtonPress-1>", self._start_drag)
        drag_bar.bind("<B1-Motion>", self._do_drag)

        # â”€â”€ Canvas (created after drag bar, fills remaining space) â”€â”€
        self._canvas = tk.Canvas(
            self._root,
            bg=self.BG_COLOR,
            highlightthickness=0,
        )
        self._canvas.pack(fill="both", expand=True)
        self._canvas.bind("<Configure>", lambda e: self._redraw_text())

        # æŒ‰ä¸‹æ™‚è¨˜éŒ„èµ·å§‹ç‹€æ…‹ï¼Œmotion/release æ”¹ç¶åˆ° rootï¼ˆæ‹–å‡ºä¸‰è§’å½¢å¾Œä»æŒçºŒè¿½è¹¤ï¼‰
        self._canvas.tag_bind("resize_handle", "<ButtonPress-1>", self._start_resize)

        # â”€â”€ å·¥å…·åˆ— (created after canvas so it has higher z-order) â”€â”€
        toolbar = tk.Frame(self._root, bg=self.TOOLBAR_BG, height=self.TOOLBAR_HEIGHT)
        toolbar.place(x=0, y=0, relwidth=1.0, height=self.TOOLBAR_HEIGHT)
        toolbar.place_forget()
        self._toolbar = toolbar

        self._dir_btn_var = tk.StringVar(value="[ENâ†’ZH â‡„]")
        tk.Button(
            toolbar,
            textvariable=self._dir_btn_var,
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._toggle_direction,
        ).pack(side="left", padx=4, pady=2)

        self._src_btn_var = tk.StringVar(value="[ğŸ”Š MON]")
        tk.Button(
            toolbar,
            textvariable=self._src_btn_var,
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._switch_source,
        ).pack(side="left", padx=4, pady=2)

        tk.Button(
            toolbar,
            text="âœ•",
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._do_close,
        ).pack(side="right", padx=4, pady=2)

        self._toolbar_hide_id = None

        # å·¥å…·åˆ—ç”±æ‹–æ‹‰æ¢è§¸ç™¼ï¼ˆhover æ‹–æ‹‰æ¢ â†’ å·¥å…·åˆ—å±•é–‹ä¸¦è¦†è“‹æ‹–æ‹‰æ¢ï¼‰
        # å·¥å…·åˆ—æœ¬èº«ä¹Ÿæ”¯æ´æ‹–æ‹‰ï¼ˆæŒ‰ä½å·¥å…·åˆ—ç©ºç™½è™•æ‹–å‹•ï¼‰
        drag_bar.bind("<Enter>", self._show_toolbar)
        drag_bar.bind("<Leave>", self._hide_toolbar)
        self._toolbar.bind("<Enter>", self._show_toolbar)
        self._toolbar.bind("<Leave>", self._hide_toolbar)
        self._toolbar.bind("<ButtonPress-1>", self._start_drag)
        self._toolbar.bind("<B1-Motion>", self._do_drag)

        self._en_str = ""
        self._zh_str = ""
        self._drag_x = 0
        self._drag_y = 0
        self._resize_start = None   # (mouse_x, mouse_y, win_w, win_h)

        self._root.bind("<Escape>", lambda e: self._do_close())
        self._root.bind("<F9>", lambda e: self._toggle_direction())
        self._root.protocol("WM_DELETE_WINDOW", self._do_close)

    def _do_close(self):
        """é—œé–‰è¦–çª—ã€‚"""
        self._root.destroy()

    def _show_toolbar(self, event=None):
        if self._toolbar_hide_id:
            self._root.after_cancel(self._toolbar_hide_id)
            self._toolbar_hide_id = None
        self._toolbar.place(x=0, y=0, relwidth=1.0, height=self.TOOLBAR_HEIGHT)
        self._toolbar.lift()

    def _hide_toolbar(self, event=None):
        self._toolbar_hide_id = self._root.after(
            400, lambda: self._toolbar.place_forget()
        )

    def _start_drag(self, event):
        self._drag_x = event.x_root - self._root.winfo_x()
        self._drag_y = event.y_root - self._root.winfo_y()

    def _do_drag(self, event):
        nx = event.x_root - self._drag_x
        ny = event.y_root - self._drag_y
        self._root.geometry(f"+{nx}+{ny}")

    def _draw_resize_handle(self):
        """Draw a small triangle at bottom-right of canvas for resizing."""
        self._canvas.delete("resize_handle")
        w = self._canvas.winfo_width() or self._root.winfo_width()
        h = self._canvas.winfo_height() or self._root.winfo_height()
        s = self.RESIZE_SIZE
        self._canvas.create_polygon(
            w, h - s,
            w - s, h,
            w, h,
            fill="#aaaaaa", outline="", tags="resize_handle",
        )
        self._canvas.tag_bind("resize_handle", "<Enter>",
                              lambda e: self._canvas.configure(cursor="sizing"))
        self._canvas.tag_bind("resize_handle", "<Leave>",
                              lambda e: self._canvas.configure(cursor=""))

    def _start_resize(self, event):
        self._resize_start = (
            event.x_root, event.y_root,
            self._root.winfo_width(), self._root.winfo_height(),
        )
        # ç¶åˆ° rootï¼Œæ‹–å‡ºä¸‰è§’å½¢ç¯„åœå¾Œä»å¯æŒçºŒç¸®æ”¾
        self._root.bind("<B1-Motion>",       self._do_resize)
        self._root.bind("<ButtonRelease-1>", self._stop_resize)
        return "break"

    def _do_resize(self, event):
        if not self._resize_start:
            return
        mx0, my0, w0, h0 = self._resize_start
        new_w = max(300, w0 + event.x_root - mx0)
        new_h = max(80,  h0 + event.y_root - my0)
        x = self._root.winfo_x()
        y = self._root.winfo_y()
        self._root.geometry(f"{new_w}x{new_h}+{x}+{y}")

    def _stop_resize(self, event):
        self._resize_start = None
        self._root.unbind("<B1-Motion>")
        self._root.unbind("<ButtonRelease-1>")

    def _toggle_direction(self):
        if self._on_toggle_direction:
            new_dir = self._on_toggle_direction()
            self.update_direction_label(new_dir)

    def update_direction_label(self, direction: str):
        label = f"[{direction} â‡„]"
        self._root.after(0, lambda: self._dir_btn_var.set(label))

    def _switch_source(self):
        if self._on_switch_source:
            self._on_switch_source()

    def update_source_label(self, source: str):
        label = "[ğŸ¤ MIC]" if source == "mic" else "[ğŸ”Š MON]"
        self._root.after(0, lambda: self._src_btn_var.set(label))

    def set_text(self, original: str = "", translated: str = ""):
        """å¾ä»»æ„åŸ·è¡Œç·’å®‰å…¨åœ°æ›´æ–°å­—å¹•ï¼ˆç”¨ after() æ’ç¨‹åˆ°ä¸»åŸ·è¡Œç·’ï¼‰ã€‚"""
        def _update():
            self._en_str = original[-120:] if len(original) > 120 else original
            self._zh_str = translated[-60:] if len(translated) > 60 else translated
            self._redraw_text()
        self._root.after(0, _update)

    def _redraw_text(self):
        """Clear canvas and re-draw subtitle text with shadow."""
        self._canvas.delete("text")

        w = self._canvas.winfo_width() or self._root.winfo_width()
        wrap_w = max(200, w - 40)   # ensure positive wrap width

        # EN line â€” 20px from left, 12px from top of canvas area
        ex, ey = 20, 12
        self._canvas.create_text(ex+2, ey+2, text=self._en_str, fill=self.SHADOW_COLOR,
                                 font=self.EN_FONT, anchor="nw", width=wrap_w, tags="text")
        self._canvas.create_text(ex,   ey,   text=self._en_str, fill=self.EN_COLOR,
                                 font=self.EN_FONT, anchor="nw", width=wrap_w, tags="text")

        # ZH line â€” below EN (~30px gap covers Arial-15 line height)
        zy = ey + 30
        self._canvas.create_text(ex+2, zy+2, text=self._zh_str, fill=self.SHADOW_COLOR,
                                 font=self.ZH_FONT, anchor="nw", width=wrap_w, tags="text")
        self._canvas.create_text(ex,   zy,   text=self._zh_str, fill=self.ZH_COLOR,
                                 font=self.ZH_FONT, anchor="nw", width=wrap_w, tags="text")

        self._draw_resize_handle()

    def run(self):
        """å•Ÿå‹• tkinter mainloopï¼ˆé˜»å¡ï¼Œå¿…é ˆåœ¨ä¸»åŸ·è¡Œç·’å‘¼å«ï¼‰ã€‚"""
        self._root.mainloop()

# ---------------------------------------------------------------------------
# Audio Sources
# ---------------------------------------------------------------------------

TARGET_SR = 16000
CHUNK_SAMPLES = 8000  # 0.5 ç§’ @ 16kHz


class AudioSource(ABC):
    """éŸ³è¨Šä¾†æºæŠ½è±¡ä»‹é¢ã€‚æœªä¾†å¯æ–°å¢ MicrophoneAudioSourceã€NetworkAudioSource ç­‰ã€‚"""

    @abstractmethod
    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        """é–‹å§‹æ“·å–éŸ³è¨Šï¼Œæ¯ 0.5 ç§’ä»¥ 16kHz float32 mono ndarray å‘¼å« callbackã€‚"""

    @abstractmethod
    def stop(self) -> None:
        """åœæ­¢æ“·å–ã€‚"""

    @staticmethod
    def list_devices() -> None:
        """åˆ—å‡ºç³»çµ±éŸ³è¨Šè£ç½®ã€‚"""
        import sounddevice as sd
        print("=== éŸ³è¨Šè£ç½®æ¸…å–® ===")
        print(sd.query_devices())
        if sys.platform == "win32":
            print("\n=== WASAPI Loopback å¯ç”¨è£ç½®ï¼ˆå¯ç”¨æ–¼ --monitor-deviceï¼‰===")
            try:
                wasapi_idx = next(
                    (i for i, api in enumerate(sd.query_hostapis()) if "wasapi" in api["name"].lower()),
                    None,
                )
                if wasapi_idx is not None:
                    for i, dev in enumerate(sd.query_devices()):
                        if dev["hostapi"] == wasapi_idx and dev["max_output_channels"] > 0:
                            print(f"  [{i}] {dev['name']} "
                                  f"({dev['max_output_channels']}ch, {int(dev['default_samplerate'])}Hz)")
                else:
                    print("  ï¼ˆæ‰¾ä¸åˆ° WASAPI host APIï¼‰")
            except Exception as e:
                print(f"  ï¼ˆç„¡æ³•åˆ—å‡º WASAPI è£ç½®ï¼š{e}ï¼‰")
        else:
            print("\n=== PulseAudio Monitor Sourcesï¼ˆå¯ç”¨æ–¼ --monitor-deviceï¼‰===")
            try:
                result = subprocess.run(
                    ["pactl", "list", "sources", "short"],
                    capture_output=True, text=True, timeout=3,
                )
                for line in result.stdout.splitlines():
                    if "monitor" in line.lower():
                        print(" ", line)
            except Exception:
                print("  ï¼ˆç„¡æ³•å–å¾— PulseAudio sourcesï¼Œè«‹ç¢ºèª pactl å·²å®‰è£ï¼‰")


class MonitorAudioSource(AudioSource):
    """
    æ“·å–ç³»çµ±æ’­æ”¾éŸ³è¨Šã€‚

    - Linux:   PipeWire/PulseAudio monitor sourceï¼ˆé€é PULSE_SOURCE + ALSA pulseï¼‰
    - Windows: WASAPI Loopbackï¼ˆé€é sounddevice WasapiSettingsï¼‰

    ä½¿ç”¨ queue.Queue è§£è€¦éŸ³è¨Š callback èˆ‡ ASR HTTP è«‹æ±‚ï¼Œé¿å…
    é˜»å¡æ“ä½œæ±¡æŸ“å³æ™‚éŸ³è¨ŠåŸ·è¡Œç·’ã€‚
    """

    # Linux é è¨­ monitor sourceï¼›Windows ç‚º Noneï¼ˆè‡ªå‹•åµæ¸¬é è¨­è¼¸å‡ºè£ç½®ï¼‰
    DEFAULT_DEVICE = None if sys.platform == "win32" else "alsa_output.pci-0000_00_1f.3.iec958-stereo.monitor"
    ALSA_PULSE_DEVICE = "pulse"  # Linux onlyï¼šALSA pulse plugin

    def __init__(self, device: str | None = None):
        # Linux: PulseAudio source åç¨±ï¼ˆNone â†’ DEFAULT_DEVICEï¼‰
        # Windows: è¼¸å‡ºè£ç½®åç¨±æˆ–ç´¢å¼•ï¼ˆNone â†’ è‡ªå‹•åµæ¸¬é è¨­è¼¸å‡ºï¼‰
        self._device = device if sys.platform == "win32" else (device or self.DEFAULT_DEVICE)
        self._stream = None
        self._pa = None          # pyaudiowpatch instance (Windows only)
        self._buf: np.ndarray = np.zeros(0, dtype=np.float32)
        self._native_sr: int = 0
        self._callback: Callable[[np.ndarray], None] | None = None
        self._queue: queue.Queue = queue.Queue()
        self._running: bool = False
        self._consumer_thread: threading.Thread | None = None

    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        if self._stream is not None:
            raise RuntimeError("MonitorAudioSource is already running; call stop() first.")

        import sounddevice as sd

        self._callback = callback
        self._buf = np.zeros(0, dtype=np.float32)
        self._running = True

        if sys.platform == "win32":
            self._setup_windows(sd)
        else:
            self._setup_linux(sd)

        # æ¶ˆè²»è€…åŸ·è¡Œç·’ï¼šå¾ queue å–éŸ³è¨Šã€resampleã€é€ callback
        self._consumer_thread = threading.Thread(target=self._consumer, daemon=True)
        self._consumer_thread.start()
        self._stream.start()

    def _setup_linux(self, sd) -> None:
        """Linuxï¼šé€é PULSE_SOURCE + ALSA pulse device æ“·å– monitor sourceã€‚"""
        os.environ["PULSE_SOURCE"] = self._device
        dev_info = sd.query_devices(self.ALSA_PULSE_DEVICE, kind="input")
        self._native_sr = int(dev_info["default_samplerate"])  # é€šå¸¸ 44100 æˆ– 48000
        self._stream = sd.InputStream(
            samplerate=self._native_sr,
            channels=1,
            dtype="float32",
            blocksize=int(self._native_sr * 0.05),  # 50ms å›ºå®š buffer
            device=self.ALSA_PULSE_DEVICE,
            callback=self._sd_callback,
        )

    def _setup_windows(self, sd) -> None:
        """Windowsï¼šé€é pyaudiowpatch WASAPI Loopback æ“·å–ç³»çµ±æ’­æ”¾éŸ³è¨Šã€‚"""
        import pyaudiowpatch as pyaudio

        self._pa = pyaudio.PyAudio()
        wasapi_info = self._pa.get_host_api_info_by_type(pyaudio.paWASAPI)

        if self._device is not None:
            loopback_idx = int(self._device)
            dev_info = self._pa.get_device_info_by_index(loopback_idx)
        else:
            # è‡ªå‹•ï¼šæ‰¾é è¨­è¼¸å‡ºè£ç½®å°æ‡‰çš„ loopback è£ç½®
            default_out_idx = wasapi_info["defaultOutputDevice"]
            default_out = self._pa.get_device_info_by_index(default_out_idx)
            loopback_idx = None
            for i in range(self._pa.get_device_count()):
                dev = self._pa.get_device_info_by_index(i)
                if dev.get("isLoopbackDevice") and dev["name"].startswith(default_out["name"]):
                    loopback_idx = i
                    dev_info = dev
                    break
            if loopback_idx is None:
                raise RuntimeError(
                    f"æ‰¾ä¸åˆ° '{default_out['name']}' çš„ WASAPI Loopback è£ç½®"
                )

        self._native_sr = int(dev_info["defaultSampleRate"])
        channels = max(int(dev_info["maxInputChannels"]), 1)
        print(f"[Monitor] WASAPI Loopback: {dev_info['name']}  sr={self._native_sr}  ch={channels}", flush=True)

        def _pa_callback(in_data, frame_count, time_info, status):
            audio = np.frombuffer(in_data, dtype=np.float32)
            if channels > 1:
                audio = audio.reshape(-1, channels)[:, 0]
            self._queue.put(audio.copy())
            return (None, pyaudio.paContinue)

        pa_stream = self._pa.open(
            format=pyaudio.paFloat32,
            channels=channels,
            rate=self._native_sr,
            input=True,
            input_device_index=loopback_idx,
            frames_per_buffer=int(self._native_sr * 0.05),
            stream_callback=_pa_callback,
        )

        # åŒ…è£æˆç›¸å®¹ sounddevice ä»‹é¢çš„ç‰©ä»¶
        class _StreamWrapper:
            def __init__(self, s): self._s = s
            def start(self): self._s.start_stream()
            def stop(self): self._s.stop_stream()
            def close(self): self._s.close()

        self._stream = _StreamWrapper(pa_stream)

    def _sd_callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        """éŸ³è¨ŠåŸ·è¡Œç·’ callbackï¼šåªåšæœ€è¼•é‡çš„ enqueueï¼Œä¸åšä»»ä½•é˜»å¡æ“ä½œã€‚"""
        if status:
            print(f"[Audio] {status}")
        self._queue.put(indata[:, 0].copy())

    def _consumer(self) -> None:
        """æ¶ˆè²»è€…åŸ·è¡Œç·’ï¼šresample + ç´¯ç© buffer + å‘¼å« ASR callbackã€‚"""
        while self._running:
            try:
                raw = self._queue.get(timeout=0.1)
            except queue.Empty:
                continue

            try:
                # resample native_sr â†’ 16kHzï¼ˆåœ¨éå³æ™‚åŸ·è¡Œç·’ä¸­é€²è¡Œï¼‰
                target_len = int(len(raw) * TARGET_SR / self._native_sr)
                if target_len == 0:
                    continue
                resampled = signal.resample(raw, target_len).astype(np.float32)
                self._buf = np.concatenate([self._buf, resampled])

                # æ¯ç´¯ç© CHUNK_SAMPLES å°±é€å‡ºä¸€æ¬¡
                while len(self._buf) >= CHUNK_SAMPLES:
                    chunk = self._buf[:CHUNK_SAMPLES].copy()
                    self._buf = self._buf[CHUNK_SAMPLES:]
                    if self._callback:
                        self._callback(chunk)
            except Exception as e:
                print(f"[Consumer error] {e}", flush=True)
                import traceback; traceback.print_exc()

    def stop(self) -> None:
        self._running = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None
        if self._pa:
            self._pa.terminate()
            self._pa = None
        if self._consumer_thread:
            self._consumer_thread.join(timeout=1.0)
            self._consumer_thread = None
        self._buf = np.zeros(0, dtype=np.float32)


class MicrophoneAudioSource(AudioSource):
    """éº¥å…‹é¢¨éŸ³è¨Šä¾†æºã€‚"""

    def __init__(self, device=None):
        self._device = device  # None = ç³»çµ±é è¨­éº¥å…‹é¢¨
        self._stream = None
        self._buf: np.ndarray = np.zeros(0, dtype=np.float32)
        self._native_sr: int = 0
        self._callback: Callable[[np.ndarray], None] | None = None
        self._queue: queue.Queue = queue.Queue()
        self._running: bool = False
        self._consumer_thread: threading.Thread | None = None

    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        if self._stream is not None:
            raise RuntimeError("MicrophoneAudioSource is already running; call stop() first.")
        import sounddevice as sd
        dev_info = sd.query_devices(self._device, kind="input")
        self._native_sr = int(dev_info["default_samplerate"])
        self._callback = callback
        self._buf = np.zeros(0, dtype=np.float32)
        self._running = True
        self._consumer_thread = threading.Thread(target=self._consumer, daemon=True)
        self._consumer_thread.start()
        self._stream = sd.InputStream(
            samplerate=self._native_sr,
            channels=1,
            dtype="float32",
            blocksize=int(self._native_sr * 0.05),
            device=self._device,
            callback=self._sd_callback,
        )
        self._stream.start()

    def _sd_callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        if status:
            print(f"[Audio] {status}")
        self._queue.put(indata[:, 0].copy())

    def _consumer(self) -> None:
        while self._running:
            try:
                raw = self._queue.get(timeout=0.1)
            except queue.Empty:
                continue
            target_len = int(len(raw) * TARGET_SR / self._native_sr)
            resampled = signal.resample(raw, target_len).astype(np.float32)
            self._buf = np.concatenate([self._buf, resampled])
            while len(self._buf) >= CHUNK_SAMPLES:
                chunk = self._buf[:CHUNK_SAMPLES].copy()
                self._buf = self._buf[CHUNK_SAMPLES:]
                if self._callback:
                    self._callback(chunk)

    def stop(self) -> None:
        self._running = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None
        if self._consumer_thread:
            self._consumer_thread.join(timeout=1.0)
            self._consumer_thread = None
        self._buf = np.zeros(0, dtype=np.float32)

# ---------------------------------------------------------------------------
# Worker Processï¼ˆéŸ³è¨Š + ASR + ç¿»è­¯ï¼Œç„¡ X11ï¼‰
# ---------------------------------------------------------------------------

def _worker_main(text_q: multiprocessing.SimpleQueue, cmd_q: multiprocessing.SimpleQueue, cfg: dict) -> None:
    """
    åœ¨ç¨ç«‹ subprocess åŸ·è¡Œï¼šsounddevice + VAD + ASR + ç¿»è­¯ã€‚
    å®Œå…¨ä¸ä½¿ç”¨ X11/tkinterï¼Œé¿å…èˆ‡ä¸»ç¨‹åºçš„ XCB è¡çªã€‚

    text_q: é€å‡º {"original": str, "translated": str} æˆ– {"direction": str}
    cmd_q:  æ¥æ”¶ "toggle"ï¼ˆåˆ‡æ›ç¿»è­¯æ–¹å‘ï¼‰æˆ– "stop"

    æ¶æ§‹ï¼š
    - on_chunkï¼šéé˜»å¡ï¼ŒåªæŠŠéŸ³è¨Šæ”¾å…¥ _vad_q
    - vad_loopï¼šSilero VAD åµæ¸¬èªéŸ³/éœéŸ³ï¼Œç´¯ç©èªéŸ³ç‰‡æ®µï¼Œ
                éœéŸ³ ~0.8s å¾ŒæŠŠå®Œæ•´èªéŸ³æ”¾å…¥ _speech_q
    - asr_loopï¼šç­‰å¾… _speech_qï¼Œé€åˆ° ASR serverï¼Œæ›´æ–°å­—å¹•
    """
    import onnxruntime as ort
    from pathlib import Path
    import opencc

    os.environ.pop("DISPLAY", None)

    # ç°¡é«”â†’å°ç£ç¹é«”è½‰æ›å™¨ï¼ˆs2twp åŒ…å«è©å½™æ›¿æ›ï¼Œå¦‚ã€Œè»Ÿä»¶â†’è»Ÿé«”ã€ï¼‰
    _s2tw = opencc.OpenCC("s2twp")

    current_original = ""

    def on_translation(translated: str) -> None:
        text_q.put({"original": current_original, "translated": translated})

    debouncer = TranslationDebouncer(
        api_key=cfg["openai_api_key"],
        callback=on_translation,
        model=cfg["translation_model"],
    )
    debouncer.set_direction(cfg["direction"])

    if cfg["source"] == "monitor":
        audio_source = MonitorAudioSource(device=cfg["monitor_device"])
    else:
        audio_source = MicrophoneAudioSource(device=cfg.get("mic_device"))

    asr = ASRClient(cfg["asr_server"])

    # Silero VAD å¸¸æ•¸ï¼ˆv6 æ¨¡å‹ï¼‰
    VAD_CHUNK = 576               # 36ms @ 16kHz
    VAD_THRESHOLD = 0.5
    RT_SILENCE_CHUNKS = 22        # 0.8s - éœéŸ³å¾Œè§¸ç™¼è½‰éŒ„ï¼ˆåŒ QwenASRMiniToolï¼‰
    RT_MAX_BUFFER_CHUNKS = 277    # 10s  - å¼·åˆ¶ flushï¼ˆHTTP timeout 20s é™åˆ¶ï¼Œå–ä¸€åŠï¼‰

    # è¼‰å…¥ VAD æ¨¡å‹
    _vad_model_path = Path(__file__).parent / "silero_vad_v6.onnx"
    vad_sess = ort.InferenceSession(str(_vad_model_path))

    _vad_q: queue.Queue = queue.Queue()
    # _speech_q å‚³é€ (audio: np.ndarray, event: str)
    # event = "probe" - çŸ­éœéŸ³ï¼Œæª¢æŸ¥æ˜¯å¦å¥æœ«å†æ±ºå®šè¦ä¸è¦é¡¯ç¤º
    # event = "force" - å¼·åˆ¶ flushï¼ˆé•·éœéŸ³æˆ– max bufferï¼‰
    _speech_q: queue.Queue = queue.Queue()
    _stop_event = threading.Event()

    def on_chunk(audio: np.ndarray) -> None:
        """éé˜»å¡ï¼šåªæŠŠéŸ³è¨Šæ”¾å…¥ VAD ä½‡åˆ—ã€‚"""
        _vad_q.put(audio)

    def vad_loop() -> None:
        """
        VAD åŸ·è¡Œç·’ï¼šéœéŸ³åµæ¸¬ã€‚

        èªéŸ³çµæŸï¼ˆéœéŸ³ â‰¥ 0.8sï¼‰æˆ– buffer é” 10s ä¸Šé™æ™‚ï¼ŒæŠŠæ•´æ®µèªéŸ³é€åˆ° _speech_qã€‚
        """
        h = np.zeros((1, 1, 128), dtype=np.float32)
        c = np.zeros((1, 1, 128), dtype=np.float32)
        buf: list[np.ndarray] = []
        sil_cnt = 0
        leftover = np.zeros(0, dtype=np.float32)

        try:
            while not _stop_event.is_set():
                try:
                    audio = _vad_q.get(timeout=0.1)
                except queue.Empty:
                    continue

                audio = np.concatenate([leftover, audio])
                n_chunks = len(audio) // VAD_CHUNK
                leftover = audio[n_chunks * VAD_CHUNK:]

                for i in range(n_chunks):
                    chunk = audio[i * VAD_CHUNK:(i + 1) * VAD_CHUNK]
                    inp = chunk[np.newaxis, :].astype(np.float32)
                    out = vad_sess.run(
                        ["speech_probs", "hn", "cn"],
                        {"input": inp, "h": h, "c": c},
                    )
                    prob, h, c = out
                    prob = float(prob.flatten()[0])

                    if prob >= VAD_THRESHOLD:
                        buf.append(chunk)
                        sil_cnt = 0
                    elif buf:
                        buf.append(chunk)
                        sil_cnt += 1
                        if sil_cnt >= RT_SILENCE_CHUNKS:
                            # éœéŸ³ 0.8sï¼šé€å‡ºæ•´æ®µèªéŸ³ï¼Œä¿ç•™ h/c ä»¥å…ä¸‹å¥é–‹é ­è¢«æ¼åµæ¸¬
                            seg = np.concatenate(buf)
                            print(f"[VAD] flush silence {len(seg)/TARGET_SR:.2f}s", flush=True)
                            _speech_q.put(seg)
                            buf = []
                            sil_cnt = 0

                    # Max buffer 10sï¼šå¼·åˆ¶é€å‡ºï¼Œä¿ç•™ h/c
                    if len(buf) >= RT_MAX_BUFFER_CHUNKS:
                        seg = np.concatenate(buf)
                        print(f"[VAD] flush max {len(seg)/TARGET_SR:.2f}s", flush=True)
                        _speech_q.put(seg)
                        buf = []
                        sil_cnt = 0

        except Exception as e:
            print(f"[VAD fatal error] {e}", flush=True)
            import traceback; traceback.print_exc()

    def _to_traditional(text: str, language: str) -> str:
        """è‹¥èªè¨€ç‚ºä¸­æ–‡ï¼ˆèªè¨€æ¨™ç±¤æˆ–æ–‡å­—å…§å« CJKï¼‰ï¼Œå°‡ç°¡é«”è½‰æˆå°ç£ç¹é«”ã€‚"""
        is_chinese = (
            (language and any(kw in language.lower() for kw in ("chinese", "mandarin", "cantonese")))
            or any("\u4e00" <= c <= "\u9fff" for c in text)
        )
        if is_chinese:
            return _s2tw.convert(text)
        return text

    def asr_loop() -> None:
        """ASR åŸ·è¡Œç·’ï¼šone-shot è½‰éŒ„ï¼Œæ”¶åˆ°æ•´æ®µèªéŸ³å°±ç›´æ¥é€ server è¾¨è­˜ã€‚"""
        nonlocal current_original
        print("[ASR] thread started", flush=True)

        while not _stop_event.is_set():
            try:
                audio = _speech_q.get(timeout=0.5)
            except queue.Empty:
                continue

            if len(audio) < TARGET_SR // 8:   # < 0.125sï¼Œè·³é
                continue

            try:
                result = asr.transcribe(audio)
                language = result.get("language", "")
                text = _to_traditional(result.get("text", ""), language)
                print(f"[ASR] lang={language!r} text={text!r} same={text == current_original}", flush=True)

                if text and text != current_original:
                    current_original = text
                    text_q.put({"original": text, "translated": ""})
                    debouncer.update(text)  # ç¿»è­¯é–‹å•Ÿ

            except Exception as e:
                print(f"[Worker ASR error] {e}", flush=True)
                # timeout å¾Œæ¸…ç©ºç©å£“çš„èˆŠ chunkï¼Œé¿å… server æŒçºŒéè¼‰
                if "timed out" in str(e).lower():
                    drained = 0
                    while not _speech_q.empty():
                        try:
                            _speech_q.get_nowait()
                            drained += 1
                        except queue.Empty:
                            break
                    if drained:
                        print(f"[ASR] Cleared {drained} stale chunks after timeout", flush=True)

    vad_thread = threading.Thread(target=vad_loop, daemon=True, name="vad-thread")
    asr_thread = threading.Thread(target=asr_loop, daemon=True, name="asr-thread")
    vad_thread.start()
    asr_thread.start()

    audio_source.start(on_chunk)
    print("[Worker] Audio capture started.", flush=True)

    try:
        while True:
            if not cmd_q.empty():
                cmd = cmd_q.get()
                if cmd == "toggle":
                    new_dir = debouncer.toggle_direction()
                    text_q.put({"direction": new_dir})
                elif cmd == "switch_source":
                    audio_source.stop()
                    if isinstance(audio_source, MonitorAudioSource):
                        audio_source = MicrophoneAudioSource(device=cfg.get("mic_device"))
                        src_name = "mic"
                    else:
                        audio_source = MonitorAudioSource(device=cfg["monitor_device"])
                        src_name = "monitor"
                    audio_source.start(on_chunk)
                    text_q.put({"source": src_name})
                elif cmd == "stop":
                    break
            else:
                time.sleep(0.1)
    finally:
        _stop_event.set()
        audio_source.stop()
        debouncer.shutdown()
        vad_thread.join(timeout=3)
        asr_thread.join(timeout=5)
        print("[Worker] Stopped.", flush=True)


# ---------------------------------------------------------------------------
# Main Entry Point
# ---------------------------------------------------------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="Real-time subtitle overlay")
    parser.add_argument("--asr-server", default="http://localhost:8000",
                        help="Qwen3-ASR streaming server URL")
    parser.add_argument("--openai-api-key", default=os.environ.get("OPENAI_API_KEY", ""),
                        help="OpenAI API key (or set OPENAI_API_KEY env var)")
    parser.add_argument("--screen", type=int, default=0,
                        help="Display screen index (0=primary, 1=secondary)")
    parser.add_argument("--list-devices", action="store_true",
                        help="List available audio devices and exit")
    parser.add_argument("--translation-model", default="gpt-4o-mini",
                        help="OpenAI model for translation")
    parser.add_argument("--source", choices=["monitor", "mic"], default="monitor",
                        help="Audio source: monitorï¼ˆç³»çµ±éŸ³è¨Šï¼‰or micï¼ˆéº¥å…‹é¢¨ï¼‰")
    parser.add_argument("--monitor-device", default=MonitorAudioSource.DEFAULT_DEVICE,
                        help="éŸ³è¨Šæ“·å–è£ç½®ï¼šLinux=PulseAudio monitor source åç¨±ï¼›"
                             "Windows=WASAPI è¼¸å‡ºè£ç½®åç¨±æˆ–ç´¢å¼•ï¼ˆNone=è‡ªå‹•åµæ¸¬é è¨­è¼¸å‡ºï¼‰ã€‚"
                             "ç”¨ --list-devices æŸ¥è©¢å¯ç”¨è£ç½®")
    parser.add_argument("--mic-device", default=None,
                        help="éº¥å…‹é¢¨è£ç½®åç¨±æˆ–ç´¢å¼•ï¼ˆNone = ç³»çµ±é è¨­éº¥å…‹é¢¨ï¼‰")
    parser.add_argument("--direction", choices=["enâ†’zh", "zhâ†’en"], default="enâ†’zh",
                        help="Initial translation direction")
    args = parser.parse_args()

    if args.list_devices:
        AudioSource.list_devices()
        return

    if not args.openai_api_key:
        print("Error: --openai-api-key æˆ– OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸å¿…é ˆè¨­å®š")
        return

    cfg = {
        "asr_server": args.asr_server,
        "openai_api_key": args.openai_api_key,
        "translation_model": args.translation_model,
        "source": args.source,
        "monitor_device": args.monitor_device,
        "mic_device": args.mic_device,
        "direction": args.direction,
    }

    # æº–å‚™ IPC queuesï¼ˆç”¨ SimpleQueueï¼Œä¸æœƒåœ¨ä¸»ç¨‹åºç”¢ç”Ÿ feeder èƒŒæ™¯åŸ·è¡Œç·’ï¼‰
    text_q: multiprocessing.SimpleQueue = multiprocessing.SimpleQueue()
    cmd_q: multiprocessing.SimpleQueue = multiprocessing.SimpleQueue()

    # æœ¬åœ°æ–¹å‘è¿½è¹¤ï¼ˆUI ç”¨ï¼Œèˆ‡ worker åŒæ­¥ï¼‰
    current_direction = [args.direction]

    def on_toggle() -> str:
        current_direction[0] = "zhâ†’en" if current_direction[0] == "enâ†’zh" else "enâ†’zh"
        cmd_q.put("toggle")
        return current_direction[0]

    def on_switch_source() -> None:
        cmd_q.put("switch_source")

    # å…ˆå»ºç«‹ tkinterï¼ˆåœ¨ fork ä¹‹å‰å®Œæˆ X11 é€£ç·šï¼Œchild ç¹¼æ‰¿ fd ä½†ç«‹å³ç§»é™¤ DISPLAYï¼‰
    overlay = SubtitleOverlay(
        screen_index=args.screen,
        on_toggle_direction=on_toggle,
        on_switch_source=on_switch_source,
    )
    overlay.update_direction_label(args.direction)

    # tkinter åˆå§‹åŒ–å¾Œæ‰ fork workerï¼ˆchild ä¸ä½¿ç”¨ X11ï¼‰
    worker = multiprocessing.Process(
        target=_worker_main, args=(text_q, cmd_q, cfg),
        daemon=True, name="subtitle-worker",
    )
    worker.start()

    # ç”¨ tkinter after() è¼ªè©¢ text_qï¼ˆå…¨åœ¨ä¸»åŸ·è¡Œç·’ï¼Œé›¶ X11 ç«¶çˆ­ï¼‰
    _last_translated = [""]  # ä¿ç•™ä¸Šä¸€ç­†ç¿»è­¯ï¼Œç›´åˆ°æ–°ç¿»è­¯åˆ°ä¾†æ‰æ›¿æ›

    def poll() -> None:
        while not text_q.empty():
            msg = text_q.get()
            if "direction" in msg:
                overlay.update_direction_label(msg["direction"])
            elif "source" in msg:
                overlay.update_source_label(msg["source"])
            else:
                translated = msg.get("translated", "")
                if translated:
                    _last_translated[0] = translated
                overlay.set_text(
                    original=msg.get("original", ""),
                    translated=_last_translated[0],
                )
        overlay._root.after(50, poll)

    overlay._root.after(50, poll)
    overlay.run()  # blockingï¼Œç›´åˆ°è¦–çª—é—œé–‰

    # è¦–çª—é—œé–‰å¾Œåœæ­¢ worker
    cmd_q.put("stop")
    worker.join(timeout=3)
    if worker.is_alive():
        worker.terminate()


if __name__ == "__main__":
    # spawnï¼šå…¨æ–° Python ç¨‹åºï¼Œä¸ç¹¼æ‰¿ X11 socket fdï¼Œé¿å… XCB åºè™Ÿè¡çª
    multiprocessing.set_start_method("spawn")
    main()
